{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbfaa75c",
   "metadata": {},
   "source": [
    "This script is the refined version of your json_convert_chunk.py. It reads combined.json, maps its fields to our standard format, chunks the descriptions, and saves it with the exact same final columns as the Nyaaya script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff66d838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Kaggle Data: /home/aditya/0Legal_Agent_Project/raw_data_and_scrap_codes/combined.json ---\n",
      "Starting standardization and chunking for 1931 documents...\n",
      "Processing complete. Created 4655 final chunks.\n",
      "Success! Standardized data saved to /home/aditya/0Legal_Agent_Project/cleaned_final_csv_data/kaggle_cleaned_chunks.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_JSON_PATH = '/home/aditya/0Legal_Agent_Project/raw_data_and_scrap_codes/combined.json'\n",
    "OUTPUT_CSV_PATH = '/home/aditya/0Legal_Agent_Project/cleaned_final_csv_data/kaggle_cleaned_chunks.csv'\n",
    "\n",
    "def process_kaggle_json(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Reads the raw Kaggle JSON file, harmonizes its structure,\n",
    "    chunks the content, and saves it to a standardized, clean CSV file.\n",
    "    \"\"\"\n",
    "    print(f\"--- Processing Kaggle Data: {input_path} ---\")\n",
    "    try:\n",
    "        # For the 'combined.json' which is a JSON array of objects\n",
    "        df_raw = pd.read_json(input_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file not found at '{input_path}'.\")\n",
    "        return\n",
    "    except ValueError: # If JSON is not a simple array\n",
    "        print(f\"Error: Could not parse JSON. Ensure '{input_path}' is a valid JSON array of objects.\")\n",
    "        return\n",
    "\n",
    "    processed_chunks = []\n",
    "\n",
    "    print(f\"Starting standardization and chunking for {len(df_raw)} documents...\")\n",
    "\n",
    "    for index, row in df_raw.iterrows():\n",
    "        # --- Harmonization Step ---\n",
    "        # Map the raw JSON fields to our standard names\n",
    "        # We check for both 'Name'/'Section' and 'title'/'description' structures\n",
    "        if 'Name' in row and 'Section' in row and 'Section_Title' in row: # Structure from original combined.json\n",
    "            original_title = f\"{row.get('Name', '')} - Section {row.get('Section', '')}: {row.get('Section_Title', '')}\"\n",
    "            long_text = row.get('Description', '')\n",
    "            category = row.get('Name', 'Uncategorized Law')\n",
    "        else: # Structure from the sample you showed later\n",
    "            original_title = row.get('title', 'Untitled Section')\n",
    "            long_text = row.get('description', '')\n",
    "            category = 'Motor Vehicles Act, 1988' # Assign a default for this file type\n",
    "\n",
    "        source = f\"Kaggle Dataset - Doc {index}\"\n",
    "\n",
    "        # --- Chunking Step ---\n",
    "        chunks = re.split(r'\\n\\s*\\n', str(long_text))\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_text = chunk.strip()\n",
    "            if chunk_text:\n",
    "                chunk_id = f\"kaggle_{index}_chunk_{i}\"\n",
    "\n",
    "                processed_chunks.append({\n",
    "                    'chunk_id': chunk_id,\n",
    "                    'chunk_text': chunk_text,\n",
    "                    'source': source,\n",
    "                    'category': category,\n",
    "                    'title': original_title\n",
    "                })\n",
    "\n",
    "    df_final = pd.DataFrame(processed_chunks)\n",
    "    \n",
    "    # Reorder columns to match the other script's output\n",
    "    final_columns = ['chunk_id', 'chunk_text', 'source', 'category', 'title']\n",
    "    df_final = df_final[final_columns]\n",
    "\n",
    "    print(f\"Processing complete. Created {len(df_final)} final chunks.\")\n",
    "    \n",
    "    df_final.to_csv(output_path, index=False)\n",
    "    print(f\"Success! Standardized data saved to {output_path}\")\n",
    "\n",
    "\n",
    "# --- RUN THE SCRIPT ---\n",
    "if __name__ == '__main__':\n",
    "    process_kaggle_json(INPUT_JSON_PATH, OUTPUT_CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff4d7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvGenAI1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
