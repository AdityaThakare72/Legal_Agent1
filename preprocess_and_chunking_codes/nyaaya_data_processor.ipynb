{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc9e1ea",
   "metadata": {},
   "source": [
    "This script will now handle the nyaaya_data.csv file. It reads the raw data, chunks the content, and standardizes the output columns to our agreed-upon format (chunk_id, chunk_text, source, category, title). Notice I've added a step to try and infer a category from the URL, which is a small but intelligent refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok1\n"
     ]
    }
   ],
   "source": [
    "print(\"ok1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91114e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Nyaaya Data: /home/aditya/0Legal_Agent_Project/raw_data_and_scrap_codes/nyaaya_data.csv ---\n",
      "Starting standardization and chunking for 68 documents...\n",
      "Processing complete. Created 68 final chunks.\n",
      "Success! Standardized data saved to /home/aditya/0Legal_Agent_Project/cleaned_final_csv_data/nyaaya_cleaned_chunks.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_CSV_PATH = '/home/aditya/0Legal_Agent_Project/raw_data_and_scrap_codes/nyaaya_data.csv'\n",
    "OUTPUT_CSV_PATH = '/home/aditya/0Legal_Agent_Project/cleaned_final_csv_data/nyaaya_cleaned_chunks.csv'\n",
    "\n",
    "def process_nyaaya_csv(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Reads the raw scraped CSV from Nyaaya, standardizes its columns,\n",
    "    chunks the content, and saves it to a clean CSV file.\n",
    "    \"\"\"\n",
    "    print(f\"--- Processing Nyaaya Data: {input_path} ---\")\n",
    "    try:\n",
    "        df_raw = pd.read_csv(input_path)\n",
    "        # Ensure the content column is treated as a string to avoid errors\n",
    "        df_raw['content'] = df_raw['content'].astype(str)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file not found at '{input_path}'.\")\n",
    "        return\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: A required column is missing from the CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    # This list will hold our final, processed chunk data\n",
    "    processed_chunks = []\n",
    "\n",
    "    print(f\"Starting standardization and chunking for {len(df_raw)} documents...\")\n",
    "    \n",
    "    # Iterate through each row (each scraped article)\n",
    "    for index, row in df_raw.iterrows():\n",
    "        # --- Harmonization Step ---\n",
    "        # We map the raw columns to our standard names\n",
    "        original_title = row.get('title', 'Untitled')\n",
    "        source_url = row.get('url', 'Source Not Found')\n",
    "        long_text = row.get('content', '')\n",
    "\n",
    "        # Attempt to infer a category from the URL structure (e.g., /legal-explainers/criminal-law/)\n",
    "        try:\n",
    "            path_parts = urlparse(source_url).path.split('/')\n",
    "            # Find 'legal-explainers' and take the next part as the category\n",
    "            category_index = path_parts.index('legal-explainers') + 1\n",
    "            category = path_parts[category_index].replace('-', ' ').title() if len(path_parts) > category_index else 'General'\n",
    "        except (ValueError, IndexError):\n",
    "            category = 'General' # Default category if parsing fails\n",
    "\n",
    "        # --- Chunking Step ---\n",
    "        # Split the text by paragraphs (two or more newlines)\n",
    "        chunks = re.split(r'\\n\\s*\\n', long_text)\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_text = chunk.strip()\n",
    "            if chunk_text:  # Only process non-empty chunks\n",
    "                chunk_id = f\"nyaaya_{index}_chunk_{i}\"\n",
    "\n",
    "                # Create a standardized dictionary for this chunk\n",
    "                processed_chunks.append({\n",
    "                    'chunk_id': chunk_id,\n",
    "                    'chunk_text': chunk_text,\n",
    "                    'source': source_url,\n",
    "                    'category': category,\n",
    "                    'title': original_title\n",
    "                })\n",
    "\n",
    "    # Convert the list of dictionaries to our final DataFrame\n",
    "    df_final = pd.DataFrame(processed_chunks)\n",
    "\n",
    "    # Reorder columns for consistency\n",
    "    final_columns = ['chunk_id', 'chunk_text', 'source', 'category', 'title']\n",
    "    df_final = df_final[final_columns]\n",
    "\n",
    "    print(f\"Processing complete. Created {len(df_final)} final chunks.\")\n",
    "    \n",
    "    # Save the final, clean data\n",
    "    df_final.to_csv(output_path, index=False)\n",
    "    print(f\"Success! Standardized data saved to {output_path}\")\n",
    "\n",
    "\n",
    "# --- RUN THE SCRIPT ---\n",
    "if __name__ == '__main__':\n",
    "    process_nyaaya_csv(INPUT_CSV_PATH, OUTPUT_CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b3136",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvGenAI1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
